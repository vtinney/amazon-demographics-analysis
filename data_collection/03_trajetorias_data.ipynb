{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajetorias Dataset Collection - Python Version\n",
    "\n",
    "**Purpose**: Download and process the Trajetorias dataset - environmental, epidemiological, and economic indicators for Brazilian Legal Amazon municipalities\n",
    "\n",
    "**Time Required**: 15-20 minutes\n",
    "\n",
    "**Data Source**: Zenodo repository (DOI: 10.5281/zenodo.7098053)\n",
    "\n",
    "**Deliverables**: \n",
    "- Multidimensional Poverty Index (MPI) data\n",
    "- Vector-borne disease incidence data\n",
    "- Environmental indicators (deforestation, land use)\n",
    "- Population and socioeconomic indicators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and import required packages\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "!pip install requests pandas numpy matplotlib seaborn plotly -q\n",
    "!pip install beautifulsoup4 lxml openpyxl tqdm -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üêç Python version:\", __import__('sys').version.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and helper functions\n",
    "class TrajetoriasConfig:\n",
    "    \"\"\"Configuration for Trajetorias dataset processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.amazon_states = ['AC', 'AP', 'AM', 'MA', 'MT', 'PA', 'RO', 'RR', 'TO']\n",
    "        self.zenodo_base_url = \"https://zenodo.org/records/7098053/files/\"\n",
    "        \n",
    "        # Dataset components\n",
    "        self.components = {\n",
    "            'population': 'TRAJETORIAS_DATASET_Population_indicators.csv',\n",
    "            'socioeconomic': 'TRAJETORIAS_DATASET_Socio-Economic_dimension-indicators.csv',\n",
    "            'epidemiological': 'TRAJETORIAS_DATASET_Epidemiological_dimension_indicators.csv',\n",
    "            'environmental': 'TRAJETORIAS_DATASET_Environmental_dimension_indicators.csv'\n",
    "        }\n",
    "        \n",
    "        # Metadata files\n",
    "        self.metadata_files = {\n",
    "            'population_meta': 'TRAJETORIAS_DATASET_Population_indicators_METADATA.csv',\n",
    "            'socioeconomic_meta': 'TRAJETORIAS_DATASET_Socio-Economic_dimension-indicators_METADATA.csv',\n",
    "            'epidemiological_meta': 'TRAJETORIAS_DATASET_Epidemiological_dimension_indicators_METADATA.csv',\n",
    "            'environmental_meta': 'TRAJETORIAS_DATASET_Environmental_dimension_indicators_METADATA.csv'\n",
    "        }\n",
    "        \n",
    "        # Key variables for PM2.5 analysis\n",
    "        self.key_variables = {\n",
    "            'socioeconomic': [\n",
    "                'mpi_rural', 'mpi_urban', 'mpi_general',\n",
    "                'poverty_incidence', 'poverty_intensity',\n",
    "                'income', 'education', 'employment'\n",
    "            ],\n",
    "            'epidemiological': [\n",
    "                'malaria', 'dengue', 'leishmaniasis',\n",
    "                'respiratory', 'diarrhea',\n",
    "                'health_access', 'hospital_density'\n",
    "            ],\n",
    "            'environmental': [\n",
    "                'deforestation', 'forest_cover',\n",
    "                'agriculture', 'pasture', 'urban_area',\n",
    "                'precipitation', 'temperature',\n",
    "                'road_density', 'mining'\n",
    "            ],\n",
    "            'population': [\n",
    "                'population_density', 'urban_population',\n",
    "                'migration', 'age_structure'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrajetoriasConfig()\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"Create necessary directory structure\"\"\"\n",
    "    directories = [\n",
    "        'data/raw/trajetorias',\n",
    "        'data/processed',\n",
    "        'quality_checks',\n",
    "        'outputs/tables'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"üìÅ Directory structure created\")\n",
    "\n",
    "def standardize_muni_code(code):\n",
    "    \"\"\"Standardize municipality codes to 7 digits\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    return f\"{int(code):07d}\"\n",
    "\n",
    "def save_data_with_metadata(df, filename, description=\"\"):\n",
    "    \"\"\"Save data with accompanying metadata file\"\"\"\n",
    "    # Create directory if needed\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Save the data\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"filename\": os.path.basename(filename),\n",
    "        \"created\": datetime.now().isoformat(),\n",
    "        \"rows\": len(df),\n",
    "        \"columns\": len(df.columns),\n",
    "        \"description\": description,\n",
    "        \"column_names\": list(df.columns)\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = filename.replace('.csv', '_metadata.json')\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Saved: {filename} ({len(df)} rows)\")\n",
    "\n",
    "# Setup directories\n",
    "create_directories()\n",
    "\n",
    "print(\"‚úÖ Configuration ready\")\n",
    "print(f\"üåø Amazon states: {', '.join(config.amazon_states)}\")\n",
    "print(f\"üìä Dataset components: {len(config.components)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download Trajetorias dataset files\n",
    "class TrajetoriasDownloader:\n",
    "    \"\"\"Handle downloading Trajetorias dataset from Zenodo\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; TrajetoriasAnalysis/1.0)'\n",
    "        })\n",
    "    \n",
    "    def download_file(self, url, filename):\n",
    "        \"\"\"Download a single file from Zenodo\"\"\"\n",
    "        try:\n",
    "            print(f\"‚¨áÔ∏è Downloading: {os.path.basename(filename)}...\", end=\" \")\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            # Download with progress\n",
    "            response = self.session.get(url, stream=True, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get file size\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Download with progress bar\n",
    "            with open(filename, 'wb') as f:\n",
    "                if total_size > 0:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "                else:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            \n",
    "            # Check file size\n",
    "            file_size = os.path.getsize(filename)\n",
    "            print(f\"‚úÖ Success ({file_size/1024/1024:.1f} MB)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def download_all_components(self):\n",
    "        \"\"\"Download all main dataset components\"\"\"\n",
    "        print(\"üì• Downloading Trajetorias dataset components...\\n\")\n",
    "        \n",
    "        download_results = {}\n",
    "        \n",
    "        # Download main data files\n",
    "        for component, filename in self.config.components.items():\n",
    "            url = self.config.zenodo_base_url + filename\n",
    "            local_path = f\"data/raw/trajetorias/{component}_indicators.csv\"\n",
    "            \n",
    "            success = self.download_file(url, local_path)\n",
    "            download_results[component] = success\n",
    "        \n",
    "        # Download metadata files\n",
    "        print(\"\\nüìã Downloading metadata files...\")\n",
    "        for meta_name, filename in self.config.metadata_files.items():\n",
    "            url = self.config.zenodo_base_url + filename\n",
    "            local_path = f\"data/raw/trajetorias/{meta_name}.csv\"\n",
    "            \n",
    "            self.download_file(url, local_path)\n",
    "        \n",
    "        return download_results\n",
    "\n",
    "# Initialize downloader and download files\n",
    "downloader = TrajetoriasDownloader(config)\n",
    "download_results = downloader.download_all_components()\n",
    "\n",
    "# Summary\n",
    "successful_downloads = sum(download_results.values())\n",
    "total_downloads = len(download_results)\n",
    "\n",
    "print(f\"\\nüìä Download Summary:\")\n",
    "print(f\"   Successful: {successful_downloads}/{total_downloads} main files\")\n",
    "\n",
    "if successful_downloads == total_downloads:\n",
    "    print(\"‚úÖ All downloads completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some downloads failed. Proceeding with available data...\")\n",
    "    for component, success in download_results.items():\n",
    "        status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "        print(f\"   {status} {component}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and explore dataset structure\n",
    "class TrajetoriasDataExplorer:\n",
    "    \"\"\"Explore and analyze Trajetorias dataset structure\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.raw_data = {}\n",
    "    \n",
    "    def load_component(self, component_name):\n",
    "        \"\"\"Load and explore a single component\"\"\"\n",
    "        filename = f\"data/raw/trajetorias/{component_name}_indicators.csv\"\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"‚ùå {component_name} file not found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n=== {component_name.upper()} COMPONENT ===\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            data = pd.read_csv(filename, low_memory=False)\n",
    "            \n",
    "            # Basic information\n",
    "            print(f\"üìä Dimensions: {len(data)} rows √ó {len(data.columns)} columns\")\n",
    "            \n",
    "            # Check for key columns\n",
    "            if 'code_muni' in data.columns:\n",
    "                unique_munis = data['code_muni'].nunique()\n",
    "                print(f\"üèòÔ∏è Municipalities: {unique_munis}\")\n",
    "            \n",
    "            if 'year' in data.columns:\n",
    "                years = sorted(data['year'].unique())\n",
    "                if len(years) <= 10:\n",
    "                    print(f\"üìÖ Years available: {', '.join(map(str, years))}\")\n",
    "                else:\n",
    "                    print(f\"üìÖ Years available: {min(years)}-{max(years)} ({len(years)} years)\")\n",
    "            \n",
    "            # Show column preview\n",
    "            print(f\"üìã Columns (first 10):\")\n",
    "            for i, col in enumerate(data.columns[:10]):\n",
    "                print(f\"   - {col}\")\n",
    "            \n",
    "            if len(data.columns) > 10:\n",
    "                print(f\"   ... and {len(data.columns) - 10} more columns\")\n",
    "            \n",
    "            # Standardize municipal codes if present\n",
    "            if 'code_muni' in data.columns:\n",
    "                data['code_muni'] = data['code_muni'].apply(standardize_muni_code)\n",
    "                print(\"üîç Municipal codes standardized\")\n",
    "            \n",
    "            # Store in class\n",
    "            self.raw_data[component_name] = data\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {component_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_components(self):\n",
    "        \"\"\"Load all available components\"\"\"\n",
    "        print(\"üîç Exploring Trajetorias dataset structure...\")\n",
    "        \n",
    "        for component in self.config.components.keys():\n",
    "            self.load_component(component)\n",
    "        \n",
    "        return self.raw_data\n",
    "\n",
    "# Initialize explorer and load data\n",
    "explorer = TrajetoriasDataExplorer(config)\n",
    "trajetorias_data = explorer.load_all_components()\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(trajetorias_data)} components successfully\")\n",
    "print(\"\\nüìã Available components:\")\n",
    "for component, data in trajetorias_data.items():\n",
    "    if data is not None:\n",
    "        print(f\"   ‚úÖ {component}: {len(data)} records, {len(data.columns)} variables\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {component}: Failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Filter for Amazon region and process data\n",
    "print(\"üó∫Ô∏è Loading municipality-state mappings...\")\n",
    "\n",
    "# Load state mapping from census data\n",
    "state_mapping = None\n",
    "if os.path.exists(\"data/raw/census/amazon_population_2022.csv\"):\n",
    "    try:\n",
    "        census_data = pd.read_csv(\"data/raw/census/amazon_population_2022.csv\")\n",
    "        state_mapping = census_data[['code_muni', 'abbrev_state', 'name_muni']].copy()\n",
    "        state_mapping['code_muni'] = state_mapping['code_muni'].apply(standardize_muni_code)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded state mapping for {len(state_mapping)} Amazon municipalities\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load census data: {str(e)}\")\n",
    "        print(\"Will process all municipalities.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Census data not available. Will process all municipalities.\")\n",
    "\n",
    "def process_component(data, component_name, state_mapping=None):\n",
    "    \"\"\"Process and filter component for Amazon region\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        print(f\"‚ö†Ô∏è No data available for {component_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {component_name} component...\")\n",
    "    \n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Filter for Amazon municipalities if we have state mapping\n",
    "    if state_mapping is not None and 'code_muni' in processed_data.columns:\n",
    "        amazon_munis = set(state_mapping['code_muni'])\n",
    "        \n",
    "        # Filter data\n",
    "        before_count = len(processed_data)\n",
    "        processed_data = processed_data[processed_data['code_muni'].isin(amazon_munis)]\n",
    "        after_count = len(processed_data)\n",
    "        \n",
    "        print(f\"üåø Filtered from {before_count} to {after_count} Amazon municipality records\")\n",
    "    \n",
    "    # Get most recent year if multiple years available\n",
    "    if 'year' in processed_data.columns and processed_data['year'].nunique() > 1:\n",
    "        latest_year = processed_data['year'].max()\n",
    "        before_year_filter = len(processed_data)\n",
    "        processed_data = processed_data[processed_data['year'] == latest_year]\n",
    "        after_year_filter = len(processed_data)\n",
    "        \n",
    "        print(f\"üìÖ Using most recent year: {latest_year} ({after_year_filter} records)\")\n",
    "    \n",
    "    # Add state information if available\n",
    "    if state_mapping is not None and 'code_muni' in processed_data.columns:\n",
    "        processed_data = processed_data.merge(\n",
    "            state_mapping[['code_muni', 'abbrev_state']], \n",
    "            on='code_muni', \n",
    "            how='left'\n",
    "        )\n",
    "        print(\"üèõÔ∏è Added state information\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Process all components\n",
    "processed_trajetorias = {}\n",
    "\n",
    "for component_name, data in trajetorias_data.items():\n",
    "    processed_data = process_component(data, component_name, state_mapping)\n",
    "    \n",
    "    if processed_data is not None and len(processed_data) > 0:\n",
    "        processed_trajetorias[component_name] = processed_data\n",
    "        \n",
    "        # Save processed data\n",
    "        filename = f\"data/processed/trajetorias_{component_name}_amazon.csv\"\n",
    "        save_data_with_metadata(\n",
    "            processed_data,\n",
    "            filename,\n",
    "            f\"Processed {component_name} indicators from Trajetorias dataset for Amazon region\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(processed_trajetorias)} components for Amazon region\")\n",
    "\n",
    "# Summary by component\n",
    "for component, data in processed_trajetorias.items():\n",
    "    print(f\"   üìä {component}: {len(data)} records, {len(data.columns)} variables\")\n",
    "    if 'abbrev_state' in data.columns:\n",
    "        states = data['abbrev_state'].nunique()\n",
    "        print(f\"      üèõÔ∏è Covers {states} states\")\n",
    "    if 'code_muni' in data.columns:\n",
    "        munis = data['code_muni'].nunique()\n",
    "        print(f\"      üèòÔ∏è Covers {munis} municipalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract key variables for PM2.5 analysis\n",
    "def extract_key_variables(data, component_name, key_vars):\n",
    "    \"\"\"Extract key variables from component using fuzzy matching\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîç Extracting key variables from {component_name}...\")\n",
    "    \n",
    "    # Get available columns\n",
    "    available_cols = data.columns.tolist()\n",
    "    \n",
    "    # Find matching variables (case-insensitive partial matching)\n",
    "    found_vars = []\n",
    "    for key_var in key_vars:\n",
    "        # Look for columns containing the key variable name\n",
    "        matches = [col for col in available_cols \n",
    "                  if key_var.lower() in col.lower()]\n",
    "        \n",
    "        if matches:\n",
    "            found_var = matches[0]  # Take first match\n",
    "            found_vars.append(found_var)\n",
    "            print(f\"   ‚úÖ Found: {key_var} ‚Üí {found_var}\")\n",
    "    \n",
    "    # Essential columns to always include\n",
    "    essential_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "    essential_available = [col for col in essential_cols if col in available_cols]\n",
    "    \n",
    "    # Combine essential and found variables\n",
    "    selected_cols = list(set(essential_available + found_vars))\n",
    "    \n",
    "    if found_vars:\n",
    "        extracted_data = data[selected_cols].copy()\n",
    "        print(f\"üìã Extracted {len(found_vars)} key variables + {len(essential_available)} essential columns\")\n",
    "        return extracted_data\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matching key variables found for {component_name}\")\n",
    "        return None\n",
    "\n",
    "# Extract key variables from each component\n",
    "print(\"üéØ Extracting key variables for PM2.5 analysis...\")\n",
    "\n",
    "key_indicators = {}\n",
    "\n",
    "for component_name in processed_trajetorias.keys():\n",
    "    if component_name in config.key_variables:\n",
    "        data = processed_trajetorias[component_name]\n",
    "        key_vars = config.key_variables[component_name]\n",
    "        \n",
    "        extracted = extract_key_variables(data, component_name, key_vars)\n",
    "        \n",
    "        if extracted is not None:\n",
    "            key_indicators[component_name] = extracted\n",
    "            \n",
    "            # Save key indicators\n",
    "            filename = f\"data/processed/key_indicators_{component_name}.csv\"\n",
    "            save_data_with_metadata(\n",
    "                extracted,\n",
    "                filename,\n",
    "                f\"Key {component_name} indicators for PM2.5 analysis from Trajetorias dataset\"\n",
    "            )\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted key indicators from {len(key_indicators)} components\")\n",
    "\n",
    "# Summary of key indicators\n",
    "for component, data in key_indicators.items():\n",
    "    # Count non-essential columns\n",
    "    essential_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "    indicator_cols = [col for col in data.columns if col not in essential_cols]\n",
    "    \n",
    "    print(f\"   üìä {component}: {len(indicator_cols)} indicators, {len(data)} records\")\n",
    "    if 'abbrev_state' in data.columns:\n",
    "        states = data['abbrev_state'].nunique()\n",
    "        print(f\"      üèõÔ∏è {states} states covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data quality assessment\n",
    "def assess_data_quality(data, component_name):\n",
    "    \"\"\"Comprehensive quality assessment for a component\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîç Quality assessment: {component_name}\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    quality_metrics = {\n",
    "        'total_records': len(data),\n",
    "        'total_variables': len(data.columns),\n",
    "        'unique_municipalities': data['code_muni'].nunique() if 'code_muni' in data.columns else 'N/A',\n",
    "        'years_covered': sorted(data['year'].unique().tolist()) if 'year' in data.columns else 'N/A',\n",
    "        'states_covered': data['abbrev_
