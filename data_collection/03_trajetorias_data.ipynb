{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajetorias Dataset Collection - Python Version\n",
    "\n",
    "**Purpose**: Download and process the Trajetorias dataset - environmental, epidemiological, and economic indicators for Brazilian Legal Amazon municipalities\n",
    "\n",
    "**Time Required**: 15-20 minutes\n",
    "\n",
    "**Data Source**: Zenodo repository (DOI: 10.5281/zenodo.7098053)\n",
    "\n",
    "**Deliverables**: \n",
    "- Multidimensional Poverty Index (MPI) data\n",
    "- Vector-borne disease incidence data\n",
    "- Environmental indicators (deforestation, land use)\n",
    "- Population and socioeconomic indicators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and import required packages\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "!pip install requests pandas numpy matplotlib seaborn plotly -q\n",
    "!pip install beautifulsoup4 lxml openpyxl tqdm -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üêç Python version:\", __import__('sys').version.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and helper functions\n",
    "class TrajetoriasConfig:\n",
    "    \"\"\"Configuration for Trajetorias dataset processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.amazon_states = ['AC', 'AP', 'AM', 'MA', 'MT', 'PA', 'RO', 'RR', 'TO']\n",
    "        self.zenodo_base_url = \"https://zenodo.org/records/7098053/files/\"\n",
    "        \n",
    "        # Dataset components\n",
    "        self.components = {\n",
    "            'population': 'TRAJETORIAS_DATASET_Population_indicators.csv',\n",
    "            'socioeconomic': 'TRAJETORIAS_DATASET_Socio-Economic_dimension-indicators.csv',\n",
    "            'epidemiological': 'TRAJETORIAS_DATASET_Epidemiological_dimension_indicators.csv',\n",
    "            'environmental': 'TRAJETORIAS_DATASET_Environmental_dimension_indicators.csv'\n",
    "        }\n",
    "        \n",
    "        # Metadata files\n",
    "        self.metadata_files = {\n",
    "            'population_meta': 'TRAJETORIAS_DATASET_Population_indicators_METADATA.csv',\n",
    "            'socioeconomic_meta': 'TRAJETORIAS_DATASET_Socio-Economic_dimension-indicators_METADATA.csv',\n",
    "            'epidemiological_meta': 'TRAJETORIAS_DATASET_Epidemiological_dimension_indicators_METADATA.csv',\n",
    "            'environmental_meta': 'TRAJETORIAS_DATASET_Environmental_dimension_indicators_METADATA.csv'\n",
    "        }\n",
    "        \n",
    "        # Key variables for PM2.5 analysis\n",
    "        self.key_variables = {\n",
    "            'socioeconomic': [\n",
    "                'mpi_rural', 'mpi_urban', 'mpi_general',\n",
    "                'poverty_incidence', 'poverty_intensity',\n",
    "                'income', 'education', 'employment'\n",
    "            ],\n",
    "            'epidemiological': [\n",
    "                'malaria', 'dengue', 'leishmaniasis',\n",
    "                'respiratory', 'diarrhea',\n",
    "                'health_access', 'hospital_density'\n",
    "            ],\n",
    "            'environmental': [\n",
    "                'deforestation', 'forest_cover',\n",
    "                'agriculture', 'pasture', 'urban_area',\n",
    "                'precipitation', 'temperature',\n",
    "                'road_density', 'mining'\n",
    "            ],\n",
    "            'population': [\n",
    "                'population_density', 'urban_population',\n",
    "                'migration', 'age_structure'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrajetoriasConfig()\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"Create necessary directory structure\"\"\"\n",
    "    directories = [\n",
    "        'data/raw/trajetorias',\n",
    "        'data/processed',\n",
    "        'quality_checks',\n",
    "        'outputs/tables'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"üìÅ Directory structure created\")\n",
    "\n",
    "def standardize_muni_code(code):\n",
    "    \"\"\"Standardize municipality codes to 7 digits\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    return f\"{int(code):07d}\"\n",
    "\n",
    "def save_data_with_metadata(df, filename, description=\"\"):\n",
    "    \"\"\"Save data with accompanying metadata file\"\"\"\n",
    "    # Create directory if needed\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Save the data\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"filename\": os.path.basename(filename),\n",
    "        \"created\": datetime.now().isoformat(),\n",
    "        \"rows\": len(df),\n",
    "        \"columns\": len(df.columns),\n",
    "        \"description\": description,\n",
    "        \"column_names\": list(df.columns)\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = filename.replace('.csv', '_metadata.json')\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Saved: {filename} ({len(df)} rows)\")\n",
    "\n",
    "# Setup directories\n",
    "create_directories()\n",
    "\n",
    "print(\"‚úÖ Configuration ready\")\n",
    "print(f\"üåø Amazon states: {', '.join(config.amazon_states)}\")\n",
    "print(f\"üìä Dataset components: {len(config.components)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download Trajetorias dataset files\n",
    "class TrajetoriasDownloader:\n",
    "    \"\"\"Handle downloading Trajetorias dataset from Zenodo\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; TrajetoriasAnalysis/1.0)'\n",
    "        })\n",
    "    \n",
    "    def download_file(self, url, filename):\n",
    "        \"\"\"Download a single file from Zenodo\"\"\"\n",
    "        try:\n",
    "            print(f\"‚¨áÔ∏è Downloading: {os.path.basename(filename)}...\", end=\" \")\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            # Download with progress\n",
    "            response = self.session.get(url, stream=True, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get file size\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Download with progress bar\n",
    "            with open(filename, 'wb') as f:\n",
    "                if total_size > 0:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "                else:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            \n",
    "            # Check file size\n",
    "            file_size = os.path.getsize(filename)\n",
    "            print(f\"‚úÖ Success ({file_size/1024/1024:.1f} MB)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def download_all_components(self):\n",
    "        \"\"\"Download all main dataset components\"\"\"\n",
    "        print(\"üì• Downloading Trajetorias dataset components...\\n\")\n",
    "        \n",
    "        download_results = {}\n",
    "        \n",
    "        # Download main data files\n",
    "        for component, filename in self.config.components.items():\n",
    "            url = self.config.zenodo_base_url + filename\n",
    "            local_path = f\"data/raw/trajetorias/{component}_indicators.csv\"\n",
    "            \n",
    "            success = self.download_file(url, local_path)\n",
    "            download_results[component] = success\n",
    "        \n",
    "        # Download metadata files\n",
    "        print(\"\\nüìã Downloading metadata files...\")\n",
    "        for meta_name, filename in self.config.metadata_files.items():\n",
    "            url = self.config.zenodo_base_url + filename\n",
    "            local_path = f\"data/raw/trajetorias/{meta_name}.csv\"\n",
    "            \n",
    "            self.download_file(url, local_path)\n",
    "        \n",
    "        return download_results\n",
    "\n",
    "# Initialize downloader and download files\n",
    "downloader = TrajetoriasDownloader(config)\n",
    "download_results = downloader.download_all_components()\n",
    "\n",
    "# Summary\n",
    "successful_downloads = sum(download_results.values())\n",
    "total_downloads = len(download_results)\n",
    "\n",
    "print(f\"\\nüìä Download Summary:\")\n",
    "print(f\"   Successful: {successful_downloads}/{total_downloads} main files\")\n",
    "\n",
    "if successful_downloads == total_downloads:\n",
    "    print(\"‚úÖ All downloads completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some downloads failed. Proceeding with available data...\")\n",
    "    for component, success in download_results.items():\n",
    "        status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "        print(f\"   {status} {component}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and explore dataset structure\n",
    "class TrajetoriasDataExplorer:\n",
    "    \"\"\"Explore and analyze Trajetorias dataset structure\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.raw_data = {}\n",
    "    \n",
    "    def load_component(self, component_name):\n",
    "        \"\"\"Load and explore a single component\"\"\"\n",
    "        filename = f\"data/raw/trajetorias/{component_name}_indicators.csv\"\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"‚ùå {component_name} file not found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n=== {component_name.upper()} COMPONENT ===\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            data = pd.read_csv(filename, low_memory=False)\n",
    "            \n",
    "            # Basic information\n",
    "            print(f\"üìä Dimensions: {len(data)} rows √ó {len(data.columns)} columns\")\n",
    "            \n",
    "            # Check for key columns\n",
    "            if 'code_muni' in data.columns:\n",
    "                unique_munis = data['code_muni'].nunique()\n",
    "                print(f\"üèòÔ∏è Municipalities: {unique_munis}\")\n",
    "            \n",
    "            if 'year' in data.columns:\n",
    "                years = sorted(data['year'].unique())\n",
    "                if len(years) <= 10:\n",
    "                    print(f\"üìÖ Years available: {', '.join(map(str, years))}\")\n",
    "                else:\n",
    "                    print(f\"üìÖ Years available: {min(years)}-{max(years)} ({len(years)} years)\")\n",
    "            \n",
    "            # Show column preview\n",
    "            print(f\"üìã Columns (first 10):\")\n",
    "            for i, col in enumerate(data.columns[:10]):\n",
    "                print(f\"   - {col}\")\n",
    "            \n",
    "            if len(data.columns) > 10:\n",
    "                print(f\"   ... and {len(data.columns) - 10} more columns\")\n",
    "            \n",
    "            # Standardize municipal codes if present\n",
    "            if 'code_muni' in data.columns:\n",
    "                data['code_muni'] = data['code_muni'].apply(standardize_muni_code)\n",
    "                print(\"üîç Municipal codes standardized\")\n",
    "            \n",
    "            # Store in class\n",
    "            self.raw_data[component_name] = data\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {component_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_components(self):\n",
    "        \"\"\"Load all available components\"\"\"\n",
    "        print(\"üîç Exploring Trajetorias dataset structure...\")\n",
    "        \n",
    "        for component in self.config.components.keys():\n",
    "            self.load_component(component)\n",
    "        \n",
    "        return self.raw_data\n",
    "\n",
    "# Initialize explorer and load data\n",
    "explorer = TrajetoriasDataExplorer(config)\n",
    "trajetorias_data = explorer.load_all_components()\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(trajetorias_data)} components successfully\")\n",
    "print(\"\\nüìã Available components:\")\n",
    "for component, data in trajetorias_data.items():\n",
    "    if data is not None:\n",
    "        print(f\"   ‚úÖ {component}: {len(data)} records, {len(data.columns)} variables\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {component}: Failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Filter for Amazon region and process data\n",
    "print(\"üó∫Ô∏è Loading municipality-state mappings...\")\n",
    "\n",
    "# Load state mapping from census data\n",
    "state_mapping = None\n",
    "if os.path.exists(\"data/raw/census/amazon_population_2022.csv\"):\n",
    "    try:\n",
    "        census_data = pd.read_csv(\"data/raw/census/amazon_population_2022.csv\")\n",
    "        state_mapping = census_data[['code_muni', 'abbrev_state', 'name_muni']].copy()\n",
    "        state_mapping['code_muni'] = state_mapping['code_muni'].apply(standardize_muni_code)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded state mapping for {len(state_mapping)} Amazon municipalities\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load census data: {str(e)}\")\n",
    "        print(\"Will process all municipalities.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Census data not available. Will process all municipalities.\")\n",
    "\n",
    "def process_component(data, component_name, state_mapping=None):\n",
    "    \"\"\"Process and filter component for Amazon region\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        print(f\"‚ö†Ô∏è No data available for {component_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {component_name} component...\")\n",
    "    \n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Filter for Amazon municipalities if we have state mapping\n",
    "    if state_mapping is not None and 'code_muni' in processed_data.columns:\n",
    "        amazon_munis = set(state_mapping['code_muni'])\n",
    "        \n",
    "        # Filter data\n",
    "        before_count = len(processed_data)\n",
    "        processed_data = processed_data[processed_data['code_muni'].isin(amazon_munis)]\n",
    "        after_count = len(processed_data)\n",
    "        \n",
    "        print(f\"üåø Filtered from {before_count} to {after_count} Amazon municipality records\")\n",
    "    \n",
    "    # Get most recent year if multiple years available\n",
    "    if 'year' in processed_data.columns and processed_data['year'].nunique() > 1:\n",
    "        latest_year = processed_data['year'].max()\n",
    "        before_year_filter = len(processed_data)\n",
    "        processed_data = processed_data[processed_data['year'] == latest_year]\n",
    "        after_year_filter = len(processed_data)\n",
    "        \n",
    "        print(f\"üìÖ Using most recent year: {latest_year} ({after_year_filter} records)\")\n",
    "    \n",
    "    # Add state information if available\n",
    "    if state_mapping is not None and 'code_muni' in processed_data.columns:\n",
    "        processed_data = processed_data.merge(\n",
    "            state_mapping[['code_muni', 'abbrev_state']], \n",
    "            on='code_muni', \n",
    "            how='left'\n",
    "        )\n",
    "        print(\"üèõÔ∏è Added state information\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Process all components\n",
    "processed_trajetorias = {}\n",
    "\n",
    "for component_name, data in trajetorias_data.items():\n",
    "    processed_data = process_component(data, component_name, state_mapping)\n",
    "    \n",
    "    if processed_data is not None and len(processed_data) > 0:\n",
    "        processed_trajetorias[component_name] = processed_data\n",
    "        \n",
    "        # Save processed data\n",
    "        filename = f\"data/processed/trajetorias_{component_name}_amazon.csv\"\n",
    "        save_data_with_metadata(\n",
    "            processed_data,\n",
    "            filename,\n",
    "            f\"Processed {component_name} indicators from Trajetorias dataset for Amazon region\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(processed_trajetorias)} components for Amazon region\")\n",
    "\n",
    "# Summary by component\n",
    "for component, data in processed_trajetorias.items():\n",
    "    print(f\"   üìä {component}: {len(data)} records, {len(data.columns)} variables\")\n",
    "    if 'abbrev_state' in data.columns:\n",
    "        states = data['abbrev_state'].nunique()\n",
    "        print(f\"      üèõÔ∏è Covers {states} states\")\n",
    "    if 'code_muni' in data.columns:\n",
    "        munis = data['code_muni'].nunique()\n",
    "        print(f\"      üèòÔ∏è Covers {munis} municipalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract key variables for PM2.5 analysis\n",
    "def extract_key_variables(data, component_name, key_vars):\n",
    "    \"\"\"Extract key variables from component using fuzzy matching\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîç Extracting key variables from {component_name}...\")\n",
    "    \n",
    "    # Get available columns\n",
    "    available_cols = data.columns.tolist()\n",
    "    \n",
    "    # Find matching variables (case-insensitive partial matching)\n",
    "    found_vars = []\n",
    "    for key_var in key_vars:\n",
    "        # Look for columns containing the key variable name\n",
    "        matches = [col for col in available_cols \n",
    "                  if key_var.lower() in col.lower()]\n",
    "        \n",
    "        if matches:\n",
    "            found_var = matches[0]  # Take first match\n",
    "            found_vars.append(found_var)\n",
    "            print(f\"   ‚úÖ Found: {key_var} ‚Üí {found_var}\")\n",
    "    \n",
    "    # Essential columns to always include\n",
    "    essential_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "    essential_available = [col for col in essential_cols if col in available_cols]\n",
    "    \n",
    "    # Combine essential and found variables\n",
    "    selected_cols = list(set(essential_available + found_vars))\n",
    "    \n",
    "    if found_vars:\n",
    "        extracted_data = data[selected_cols].copy()\n",
    "        print(f\"üìã Extracted {len(found_vars)} key variables + {len(essential_available)} essential columns\")\n",
    "        return extracted_data\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matching key variables found for {component_name}\")\n",
    "        return None\n",
    "\n",
    "# Extract key variables from each component\n",
    "print(\"üéØ Extracting key variables for PM2.5 analysis...\")\n",
    "\n",
    "key_indicators = {}\n",
    "\n",
    "for component_name in processed_trajetorias.keys():\n",
    "    if component_name in config.key_variables:\n",
    "        data = processed_trajetorias[component_name]\n",
    "        key_vars = config.key_variables[component_name]\n",
    "        \n",
    "        extracted = extract_key_variables(data, component_name, key_vars)\n",
    "        \n",
    "        if extracted is not None:\n",
    "            key_indicators[component_name] = extracted\n",
    "            \n",
    "            # Save key indicators\n",
    "            filename = f\"data/processed/key_indicators_{component_name}.csv\"\n",
    "            save_data_with_metadata(\n",
    "                extracted,\n",
    "                filename,\n",
    "                f\"Key {component_name} indicators for PM2.5 analysis from Trajetorias dataset\"\n",
    "            )\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted key indicators from {len(key_indicators)} components\")\n",
    "\n",
    "# Summary of key indicators\n",
    "for component, data in key_indicators.items():\n",
    "    # Count non-essential columns\n",
    "    essential_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "    indicator_cols = [col for col in data.columns if col not in essential_cols]\n",
    "    \n",
    "    print(f\"   üìä {component}: {len(indicator_cols)} indicators, {len(data)} records\")\n",
    "    if 'abbrev_state' in data.columns:\n",
    "        states = data['abbrev_state'].nunique()\n",
    "        print(f\"      üèõÔ∏è {states} states covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data quality assessment\n",
    "def assess_data_quality(data, component_name):\n",
    "    \"\"\"Comprehensive quality assessment for a component\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîç Quality assessment: {component_name}\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    quality_metrics = {\n",
    "        'total_records': len(data),\n",
    "        'total_variables': len(data.columns),\n",
    "        'unique_municipalities': data['code_muni'].nunique() if 'code_muni' in data.columns else 'N/A',\n",
    "        'years_covered': sorted(data['year'].unique().tolist()) if 'year' in data.columns else 'N/A',\n",
    "        'states_covered': data['abbrev_state'].nunique() if 'abbrev_state' in data.columns else 'N/A'
    }
    
    # Missing data analysis
    missing_data = data.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
    
    quality_metrics['missing_data_summary'] = {
        'variables_with_missing': len(missing_data),
        'total_variables': len(data.columns),
        'worst_missing_var': missing_data.index[0] if len(missing_data) > 0 else 'None',
        'worst_missing_pct': round((missing_data.iloc[0] / len(data)) * 100, 1) if len(missing_data) > 0 else 0
    }
    
    # Numeric variables summary
    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
    quality_metrics['numeric_variables'] = len(numeric_cols)
    
    # Print key findings
    print(f"   üìä Records: {quality_metrics['total_records']}")
    print(f"   üìã Variables: {quality_metrics['total_variables']}")
    print(f"   üèòÔ∏è Municipalities: {quality_metrics['unique_municipalities']}")
    
    if quality_metrics['years_covered'] != 'N/A':
        years = quality_metrics['years_covered']
        if len(years) > 3:
            years_str = f"{min(years)}-{max(years)}"
        else:
            years_str = ', '.join(map(str, years))
        print(f"   üìÖ Years: {years_str}")
    
    if quality_metrics['states_covered'] != 'N/A':
        print(f"   üèõÔ∏è States: {quality_metrics['states_covered']}/{len(config.amazon_states)}")
    
    print(f"   ‚ùì Variables with missing data: {quality_metrics['missing_data_summary']['variables_with_missing']}/{quality_metrics['missing_data_summary']['total_variables']}")
    
    if quality_metrics['missing_data_summary']['worst_missing_pct'] > 0:
        print(f"   ‚ö†Ô∏è Highest missing data: {quality_metrics['missing_data_summary']['worst_missing_var']} ({quality_metrics['missing_data_summary']['worst_missing_pct']}%)")
    
    return quality_metrics

# Run quality assessment
print("üîç Running comprehensive data quality assessment...")

quality_report = {}
for component_name, data in processed_trajetorias.items():
    metrics = assess_data_quality(data, component_name)
    if metrics:
        quality_report[component_name] = metrics

# Overall assessment
print(f"\\nüéØ Overall Trajetorias Data Assessment:")
print(f"   üì¶ Components available: {len(quality_report)}/{len(config.components)}")

if quality_report:
    total_records = sum(metrics['total_records'] for metrics in quality_report.values())
    print(f"   üìä Total records across components: {total_records}")
    
    # Municipality coverage check
    if 'socioeconomic' in quality_report:
        muni_coverage = quality_report['socioeconomic']['unique_municipalities']
        if state_mapping is not None:
            total_amazon_munis = len(state_mapping)
            coverage_pct = round((muni_coverage / total_amazon_munis) * 100, 1)
            print(f"   üåø Amazon municipality coverage: {muni_coverage}/{total_amazon_munis} ({coverage_pct}%)")

# Save quality report
quality_report_file = "quality_checks/trajetorias_quality_report.json"
with open(quality_report_file, 'w', encoding='utf-8') as f:
    json.dump(quality_report, f, indent=2, ensure_ascii=False, default=str)

print(f"\\nüíæ Quality report saved to {quality_report_file}")
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create summary visualizations\n",
    "def create_component_visualizations(data, component_name):\n",
    "    \"\"\"Create summary visualizations for a component\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        print(f\"‚ö†Ô∏è No data available for {component_name} visualizations\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìà Creating visualizations for {component_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding ID columns)\n",
    "    exclude_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "    numeric_cols = [col for col in data.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in exclude_cols]\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        print(f\"   ‚ö†Ô∏è No numeric variables found for {component_name}\")\n",
    "        return\n",
    "    \n",
    "    # Select first 4 variables for plotting\n",
    "    plot_vars = numeric_cols[:4]\n",
    "    \n",
    "    # Create subplots\n",
    "    n_plots = len(plot_vars)\n",
    "    if n_plots == 1:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        axes = [axes]\n",
    "    elif n_plots == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    elif n_plots == 3:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    fig.suptitle(f'{component_name.title()} Component - Key Variables Distribution', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, var in enumerate(plot_vars):\n",
    "        try:\n",
    "            # Remove missing values\n",
    "            plot_data = data[var].dropna()\n",
    "            \n",
    "            if len(plot_data) > 0:\n",
    "                # Create histogram\n",
    "                axes[i].hist(plot_data, bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                axes[i].set_title(f'Distribution of {var}')\n",
    "                axes[i].set_xlabel(var)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics text\n",
    "                mean_val = plot_data.mean()\n",
    "                median_val = plot_data.median()\n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.2f}')\n",
    "                axes[i].axvline(median_val, color='orange', linestyle='--', alpha=0.8, label=f'Median: {median_val:.2f}')\n",
    "                axes[i].legend()\n",
    "                \n",
    "                # Print statistics\n",
    "                print(f\"   üìä {var}: Mean={mean_val:.2f}, Median={median_val:.2f}, Range=[{plot_data.min():.2f}, {plot_data.max():.2f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Could not plot {var}: {str(e)}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error plotting {var}', \n",
    "                        transform=axes[i].transAxes, ha='center', va='center')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(plot_vars), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = f\"outputs/tables/{component_name}_summary_plots.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   üíæ Plots saved to {plot_filename}\")\n",
    "\n",
    "# Create visualizations for key indicators\n",
    "print(\"üìä Creating summary visualizations for key indicators...\")\n",
    "\n",
    "for component_name, data in key_indicators.items():\n",
    "    create_component_visualizations(data, component_name)\n",
    "\n",
    "print(\"\\n‚úÖ Summary visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create comprehensive variable summary table\n",
    "def create_variable_summary():\n",
    "    \"\"\"Create comprehensive summary table of all variables\"\"\"\n",
    "    print(\"üìã Creating comprehensive variable summary table...\")\n",
    "    \n",
    "    summary_rows = []\n",
    "    \n",
    "    for component_name, data in key_indicators.items():\n",
    "        if data is None or len(data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Exclude administrative columns\n",
    "        exclude_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "        vars_to_analyze = [col for col in data.columns if col not in exclude_cols]\n",
    "        \n",
    "        for var_name in vars_to_analyze:\n",
    "            var_data = data[var_name]\n",
    "            \n",
    "            # Basic info\n",
    "            row = {\n",
    "                'Component': component_name.title(),\n",
    "                'Variable': var_name,\n",
    "                'Type': str(var_data.dtype),\n",
    "                'Missing_Count': var_data.isnull().sum(),\n",
    "                'Missing_Percent': round((var_data.isnull().sum() / len(var_data)) * 100, 1),\n",
    "                'Non_Missing_Count': var_data.notna().sum()\n",
    "            }\n",
    "            \n",
    "            # Add descriptive statistics for numeric variables\n",
    "            if pd.api.types.is_numeric_dtype(var_data):\n",
    "                clean_data = var_data.dropna()\n",
    "                if len(clean_data) > 0:\n",
    "                    row.update({\n",
    "                        'Mean': round(clean_data.mean(), 2),\n",
    "                        'Median': round(clean_data.median(), 2),\n",
    "                        'Min': round(clean_data.min(), 2),\n",
    "                        'Max': round(clean_data.max(), 2),\n",
    "                        'Std': round(clean_data.std(), 2)\n",
    "                    })\n",
    "                else:\n",
    "                    row.update({'Mean': None, 'Median': None, 'Min': None, 'Max': None, 'Std': None})\n",
    "            else:\n",
    "                row.update({'Mean': None, 'Median': None, 'Min': None, 'Max': None, 'Std': None})\n",
    "            \n",
    "            summary_rows.append(row)\n",
    "    \n",
    "    if summary_rows:\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "        \n",
    "        print(f\"üìä Variable summary table created ({len(summary_df)} variables)\")\n",
    "        \n",
    "        # Display first 20 rows\n",
    "        print(\"\\nüìã Variable Summary (first 20 rows):\")\n",
    "        display_df = summary_df.head(20)\n",
    "        \n",
    "        # Format for better display\n",
    "        print(display_df.to_string(index=False, max_cols=8))\n",
    "        \n",
    "        # Save complete summary\n",
    "        summary_file = \"outputs/tables/trajetorias_variable_summary.csv\"\n",
    "        save_data_with_metadata(\n",
    "            summary_df,\n",
    "            summary_file,\n",
    "            \"Complete variable summary for Trajetorias dataset key indicators\"\n",
    "        )\n",
    "        \n",
    "        # Component summary\n",
    "        component_summary = summary_df.groupby('Component').agg({\n",
    "            'Variable': 'count',\n",
    "            'Missing_Percent': 'mean',\n",
    "            'Type': lambda x: (x == 'float64').sum() + (x == 'int64').sum()\n",
    "        }).round(1)\n",
    "        \n",
    "        component_summary.columns = ['Total_Variables', 'Avg_Missing_Percent', 'Numeric_Variables']\n",
    "        component_summary['Variables_Complete'] = summary_df.groupby('Component')['Missing_Percent'].apply(lambda x: (x == 0).sum())\n",
    "        \n",
    "        print(f\"\\nüìä Summary by component:\")\n",
    "        print(component_summary.to_string())\n",
    "        \n",
    "        return summary_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No variables found for summary table\")\n",
    "        return None\n",
    "\n",
    "# Create variable summary\n",
    "variable_summary = create_variable_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final summary and project status\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ TRAJETORIAS DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count deliverables\n",
    "deliverables = {\n",
    "    'processed_components': 0,\n",
    "    'key_indicator_files': 0,\n",
    "    'quality_report': False,\n",
    "    'summary_table': False,\n",
    "    'visualizations': 0\n",
    "}\n",
    "\n",
    "# Check processed files\n",
    "processed_files = [f for f in os.listdir(\"data/processed\") \n",
    "                  if f.startswith(\"trajetorias_\") and f.endswith(\"_amazon.csv\")]\n",
    "deliverables['processed_components'] = len(processed_files)\n",
    "\n",
    "# Check key indicator files\n",
    "key_files = [f for f in os.listdir(\"data/processed\") \n",
    "            if f.startswith(\"key_indicators_\") and f.endswith(\".csv\")]\n",
    "deliverables['key_indicator_files'] = len(key_files)\n",
    "\n",
    "# Check quality report\n",
    "deliverables['quality_report'] = os.path.exists(\"quality_checks/trajetorias_quality_report.json\")\n",
    "\n",
    "# Check summary table\n",
    "deliverables['summary_table'] = os.path.exists(\"outputs/tables/trajetorias_variable_summary.csv\")\n",
    "\n",
    "# Check visualizations\n",
    "if os.path.exists(\"outputs/tables\"):\n",
    "    viz_files = [f for f in os.listdir(\"outputs/tables\") if f.endswith(\"_summary_plots.png\")]\n",
    "    deliverables['visualizations'] = len(viz_files)\n",
    "\n",
    "print(f\"üìã Deliverables:\")\n",
    "print(f\"   üìä Processed components: {deliverables['processed_components']}\")\n",
    "print(f\"   üéØ Key indicator files: {deliverables['key_indicator_files']}\")\n",
    "print(f\"   üìã Quality report: {'‚úÖ' if deliverables['quality_report'] else '‚ùå'}\")\n",
    "print(f\"   üìà Variable summary: {'‚úÖ' if deliverables['summary_table'] else '‚ùå'}\")\n",
    "print(f\"   üìä Visualizations: {deliverables['visualizations']}\")\n",
    "\n",
    "# Data coverage summary\n",
    "if processed_trajetorias:\n",
    "    print(f\"\\nüåø Data Coverage Summary:\")\n",
    "    \n",
    "    for component_name, data in processed_trajetorias.items():\n",
    "        if data is not None:\n",
    "            print(f\"   {component_name.title()}: {len(data)} records\", end=\"\")\n",
    "            \n",
    "            if 'abbrev_state' in data.columns:\n",
    "                states_covered = data['abbrev_state'].nunique()\n",
    "                print(f\" | {states_covered} states\", end=\"\")\n",
    "            \n",
    "            if 'code_muni' in data.columns:\n",
    "                munis_covered = data['code_muni'].nunique()\n",
    "                print(f\" | {munis_covered} municipalities\", end=\"\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    # Key indicators summary\n",
    "    if key_indicators:\n",
    "        print(f\"\\nüéØ Key Indicators Available:\")\n",
    "        \n",
    "        for component_name, data in key_indicators.items():\n",
    "            if data is not None:\n",
    "                exclude_cols = ['code_muni', 'year', 'abbrev_state', 'name_muni']\n",
    "                indicator_count = len([col for col in data.columns if col not in exclude_cols])\n",
    "                print(f\"   {component_name.title()}: {indicator_count} indicators\")\n",
    "\n",
    "# Integration readiness\n",
    "print(f\"\\nüîó Integration Readiness:\")\n",
    "census_ready = os.path.exists(\"data/raw/census/amazon_population_2022.csv\")\n",
    "trajetorias_ready = len(processed_trajetorias) > 0\n",
    "fire_ready = os.path.exists(\"data/processed/amazon_fire_by_municipality.csv\")\n",
    "\n",
    "print(f\"   üìä Census data: {'‚úÖ Ready' if census_ready else '‚ùå Missing'}\")\n",
    "print(f\"   üåø Trajetorias data: {'‚úÖ Ready' if trajetorias_ready else '‚ùå Missing'}\")\n",
    "print(f\"   üî• Fire data: {'‚úÖ Ready' if fire_ready else '‚è≥ Next step'}\")\n",
    "\n",
    "# Next steps\n",
    "print(f\"\\nüìö Ready for next phase!\")\n",
    "print(f\"‚ñ∂Ô∏è Next: Process fire data (04_fire_data.ipynb)\")\n",
    "\n",
    "# Create final project summary\n",
    "project_summary = {\n",
    "    \"project_info\": {\n",
    "        \"title\": \"Trajetorias Dataset Collection\",\n",
    "        \"analysis_date\": datetime.now().isoformat(),\n",
    "        \"components_processed\": list(processed_trajetorias.keys()),\n",
    "        \"total_components\": len(processed_trajetorias)\n",
    "    },\n",
    "    \"deliverables\": deliverables,\n",
    "    \"data_summary\": {}\n",
    "}\n",
    "\n",
    "# Add data summary for each component\n",
    "for component_name, data in processed_trajetorias.items():\n",
    "    if data is not None:\n",
    "        project_summary[\"data_summary\"][component_name] = {\n",
    "            \"records\": len(data),\n",
    "            \"variables\": len(data.columns),\n",
    "            \"municipalities\": data['code_muni'].nunique() if 'code_muni' in data.columns else None,\n",
    "            \"states\": data['abbrev_state'].nunique() if 'abbrev_state' in data.columns else None\n",
    "        }\n",
    "\n",
    "# Save project summary\n",
    "summary_file = \"data/trajetorias_project_summary.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(project_summary, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Project summary saved to {summary_file}\")\n",
    "print(\"\\nüéâ Trajetorias data collection pipeline complete!\")\n",
    "print(\"Ready for data integration and analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
